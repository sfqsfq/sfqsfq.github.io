[{"categories":["Kubernetes"],"content":" PV 是对底层网络共享存储的抽象 PVC 是对 PV 的请求 StorageClass作为对存储资源的抽象定义，对用户设置的PVC申请屏蔽后端存储的细节，一方面减少了用户对于存储资源细节的关注，另一方面减轻了管理员手工管理PV的工作，由系统自动完成PV的创建和绑定，实现了动态的资源供应。 CSI 是 Container Storage Interface 的缩写，是一个规范，用于定义存储插件与 Kubernetes 之间的接口 PV 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 apiVersion: v1 # PV kind: PersistentVolume metadata: name: pv1 spec: # 存储容量 capacity: storage: 1Gi # 存储卷模式 volumeMode: Filesystem # 访问模式 accessModes: - ReadWriteOnce # 回收策略 persistentVolumeReclaimPolicy: Recycle # 存储类型 storageClassName: slow # 存储位置 nfs: path: /data/pv1 server: node3.sfq.me PVC 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 apiVersion: v1 # PVC kind: PersistentVolumeClaim metadata: name: pvc1 spec: # 资源请求 resources: requests: storage: 1Gi # 访问模式 accessModes: - ReadWriteOnce # 存储卷模式 # volumeMode: Filesystem # 存储类型 storageClassName: slow # PV 的选择条件 selector: matchLabels: release: \"stable\" matchExpressions: - {key: environment, operator: In, values: [dev]} StorageClass 1 2 3 4 5 6 7 8 9 10 11 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: slow # 提供者，以 kubernetes.io/ 开头 provisioner: kubernetes.io/nfs # 参数 parameters: server: \"node3.sfq.me\" share: \"/data/pv\" mountOptions: \"vers=4.1\" 动态资源供应模式 1. 定义 storageClass 参考\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 sfqfs@sfq:~/k8s/storageClass$ helm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \\ nfs.ser\u003e --set nfs.server=node3.sfq.me \\ \u003e --set nfs.path=/data/pv NAME: nfs-subdir-external-provisioner LAST DEPLOYED: Fri Mar 17 00:38:06 2023 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None sfqfs@sfq:~/k8s/storageClass$ kubectl get sc NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE local-path (default) rancher.io/local-path Delete WaitForFirstConsumer false 10d nfs-client cluster.local/nfs-subdir-external-provisioner Delete Immediate true 2m48s 2. 定义 PVC 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion: v1 # PVC kind: PersistentVolumeClaim metadata: name: pvc1 spec: # 资源请求 resources: requests: storage: 1Gi # 访问模式 accessModes: - ReadWriteOnce # 存储卷模式 # volumeMode: Filesystem # 存储类型 storageClassName: nfs-client 3. Pod使用 PVC 的存储资源 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion: v1 kind: Pod metadata: name: pod-test-pv spec: containers: - name: pod-test-pv image: busybox command: - sh - -c - sleep 36000 volumeMounts: - name: mypvc mountPath: /mnt/data volumes: - name: mypvc persistentVolumeClaim: claimName: pvc1 4. 测试 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # 当前 pv sfqfs@sfq:~/k8s/storageClass$ kubectl get pv No resources found # 创建 pvc sfqfs@sfq:~/k8s/storageClass$ kubectl create -f pvc.yaml persistentvolumeclaim/pvc1 created sfqfs@sfq:~/k8s/storageClass$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE pvc1 Bound pvc-74c60ca7-9e55-4a5d-8e1b-1d7e84ad72ba 1Gi RWO nfs-client 2s # 查看 pv sfqfs@sfq:~/k8s/storageClass$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-74c60ca7-9e55-4a5d-8e1b-1d7e84ad72ba 1Gi RWO Delete Bound default/pvc1 nfs-client 15s # 创建 pod sfqfs@sfq:~/k8s/storageClass$ kubectl create -f pod.yaml pod/pod-test-pv created sfqfs@sfq:~/k8s/storageClass$ kubectl exec -it pod/pod-test-pv -- ls /mnt/data/ aa.txt sfqfs@sfq:~/k8s/storageClass$ ssh sfq@node3.sfq.me ls /data/pv default-pvc1-pvc-74c60ca7-9e55-4a5d-8e1b-1d7e84ad72ba sfqfs@sfq:~/k8s/storageClass$ ssh sfq@node3.sfq.me ls /data/pv/default-pvc1-pvc-74c60ca7-9e55-4a5d-8e1b-1d7e84ad72ba aa.txt 静态资源供应模式 1. 手动创建 PV 1 2 3 4 5 6 7 8 9 10 11 12 kind: PersistentVolume apiVersion: v1 metadata: name: pv-sfq-nfs spec: capacity: storage: 10Gi accessModes: - ReadWriteMany nfs: server: node3.sfq.me path: /data/nfs 2. 创建 PVC 1 2 3 4 5 6 7 8 9 10 11 12 13 kind: PersistentVolumeClaim apiVersion: v1 metadata: name: pvc-sfq-nfs spec: accessModes: - ReadWriteMany resources: requests: storage: 1Gi # 静态存储管理 storageClassName 为空 storageClassName: \"\" volumeName: pv-sfq-nfs 3. Pod 使用 PVC 的存储资源 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion: v1 kind: Pod metadata: name: pod-sfq-pvc spec: containers: - name: pod-sfq-pvc image: busybox command: - sh - -c - sleep 36000 volumeMounts: - name: mypvc mountPath: /mnt/data volumes: - name: mypvc persistentVolumeClaim: claimName: pvc-sfq-nfs 4. 测试 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 # pv 创建 sfqfs@sfq:~/k8s/storageClass/static$ kubectl create -f pv.yml persistentvolume/pv-sfq-nfs created sfqfs@sfq:~/k8s/storageClass/static$ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-681662da-ec58-471c-be39-5dd17ffa1a53 1Gi RWO Delete Bound default/pvc1 nfs-client 10h pv-sfq-nfs 10Gi RWX Retain Available 3s # pvc 创建 sfqfs@sfq:~/k8s/storageClass/static$ kubectl create -f pvc.yaml persistentvolumeclaim/pvc-sfq-nfs created sfqfs@sfq:~/k8s/storageClass/static$ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE pvc1 Bound pvc-681662da-ec58-471c-be39-5dd17ffa1a53 1Gi RWO nfs-client 10h pvc-sfq-nfs Bound pv-sfq-nfs 10Gi RWX 2s # pod 使用 pvc sfqfs@sfq:~/k8s/storageClass/static$ kubectl get pod -w NAME READY STATUS RESTARTS AGE nginx-deployment-7bf89ffbcd-d29qq 1/1 Running 0 11d nginx-deployment-7bf89ffbcd-5s6sm 1/1 Running 0 10d nfs-subdir-external-provisioner-7757cd775b-hskkp 1/1 Running 0 10h pod-test-pv 1/1 Running 1 (12m ago) 10h pod-sfq-pvc 1/1 Running 0 4s # 测试 sfqfs@sfq:~/k8s/storageClass/static$ kubectl exec -it pod-sfq-pvc -- touch /mnt/data/test sfqfs@sfq:~/k8s/storageClass/static$ ssh sfq@node3.sfq.me ls -l /data/nfs/ total 0 -rw-r--r--. 1 root root 0 Mar 16 23:16 test ","description":"","tags":["Kubernetes","PV","PVC"],"title":"K8S PV PVC","uri":"/posts/k8s-pv-pvc/"},{"categories":["Kubernetes"],"content":"需要持久化数据的程序会用到 Volumes。\n日志收集需求，在应用程序的容器里面加一个 sidecar, 通过共享卷的方式，将日志收集到 sidecar 容器里面，再通过 sidecar 容器将日志收集到外部的日志收集系统。 hostPath hostPath 是最简单的 Volume 类型，它直接将 Pod 的 Volume 挂载到宿主机的目录上。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 # 创建 Pod 资源对象 sfqfs@sfq:~$ cat\u003c\u003cEOF | kubectl create -f - # pod-hostpath.yaml apiVersion: v1 kind: Pod metadata: name: pod-hostpath spec: containers: - name: busybox image: busybox command: [\"sh\", \"-c\", \"sleep 36000\"] volumeMounts: - name: tmp mountPath: /test volumes: # 将宿主的 /tmp 目录挂载到容器的 /test 目录 - name: tmp hostPath: path: /tmp nodeSelector: # 挂载到 node4 节点上的目录 node: node4 EOF # 在 Pod 中创建文件，在宿主机上查看 sfqfs@sfq:~/k8s$ kubectl exec -it pod-hostpath -- touch /test/file-create-in-pod sfqfs@sfq:~/k8s$ ssh sfq@node4.sfq.me ls /tmp file-create-in-pod systemd-private-7f857c3921084bd1ab8676ed07a91071-chronyd.service-8J5JY3 emptyDir emptyDir 是一个临时目录，当 Pod 被删除时，emptyDir 也会被删除。 用于存放 Pod 运行时的数据，比如日志文件，用于 Pod 中不同 Container 之间的数据共享。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 # 创建 Pod 资源对象，容器 busybox-1 和 busybox-2 共享一个 emptyDir 卷 # 两个容器都可以读写这个卷 sfqfs@sfq:~$ cat\u003c\u003cEOF | kubectl create -f - # pod-emptydir.yaml apiVersion: v1 kind: Pod metadata: name: pod-emptydir spec: containers: - name: busybox-1 image: busybox command: [\"sh\", \"-c\", \"sleep 36000\"] volumeMounts: - name: shared mountPath: /test-1 - name: busybox-2 image: busybox command: [\"sh\", \"-c\", \"sleep 36000\"] volumeMounts: - name: shared mountPath: /test-2 volumes: # 共享卷 - name: shared emptyDir: {} EOF sfqfs@sfq:~/k8s$ kubectl exec -it pod-emptydir -c busybox-1 -- touch /test-1/file-from-busybox-1 sfqfs@sfq:~/k8s$ kubectl exec -it pod-emptydir -c busybox-2 -- ls /test-2 file-from-busybox-1 NFS 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 # 创建 NFS Volume sfqfs@sfq:~$ cat\u003c\u003cEOF | kubectl create -f - # nfs-server.yaml apiVersion: v1 kind: Pod metadata: name: nfs-server spec: containers: - name: pod-nfs-volume image: busybox command: [\"sh\", \"-c\", \"sleep 36000\"] volumeMounts: - name: nfs-volume mountPath: /data volumes: - name: nfs-volume nfs: server: node3.sfq.me path: /data EOF sfqfs@sfq:~/k8s$ kubectl exec -it nfs-server -- df -Th Filesystem Type Size Used Available Use% Mounted on overlay overlay 50.0G 4.8G 45.2G 10% / tmpfs tmpfs 64.0M 0 64.0M 0% /dev tmpfs tmpfs 2.4G 0 2.4G 0% /sys/fs/cgroup node3.sfq.me:/data nfs4 50.0G 4.9G 45.1G 10% /data sfqfs@sfq:~/k8s$ kubectl exec -it nfs-server -- touch /data/file-create-in-pod sfqfs@sfq:~/k8s$ ssh sfq@node3.sfq.me ls /data file-create-in-pod ","description":"","tags":["Kubernetes","Volumes"],"title":"K8S Volumes","uri":"/posts/k8s-volumes/"},{"categories":["Kubernetes"],"content":"ConfigMap 供容器使用的典型用法\n生成为容器内的环境变量 设置容器启动命令的启动参数（需设置为环境变量） 以Volume的形式挂载为容器内部的文件或目录 使用YAML文件创建ConfigMap 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 # 创建 ConfigMap 资源对象 sfqfs@sfq:~$ cat\u003c\u003cEOF | kubectl create -f - # cm-appvars.yaml apiVersion: v1 kind: ConfigMap metadata: name: cm-appvars data: apploglevel: \"info\" appdatadir: /var/data EOF # 获取 configmap 对象 sfqfs@sfq:~$ kubectl get cm/cm-appvars NAME DATA AGE cm-appvars 2 68s # 打印 configmap 的详细信息 sfqfs@sfq:~$ kubectl describe cm/cm-appvars Name: cm-appvars Namespace: default Labels: \u003cnone\u003e Annotations: \u003cnone\u003e Data ==== appdatadir: ---- /var/data apploglevel: ---- info BinaryData ==== Events: \u003cnone\u003e 命令行方式创建 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # 可以指定 key, 没有指定直接使用文件名作为key # source 可以为目录，此时把当前文件jian的所有文件作为key, 文件内容作为value kubectl create configmap cm-test1 --from-file=[key=]source --from-file=[key=]source sfqfs@sfq:~/k8s/StatefulSet$ ls busybox.yaml nginx-sts.yaml statefulset.md mongo-headless-service.yaml statefulset-mongo.yaml storageclass-fast.yaml sfqfs@sfq:~/k8s/StatefulSet$ kubectl create configmap cm-files --from-file=. configmap/cm-files created sfqfs@sfq:~/k8s/StatefulSet$ kubectl get cm/cm-files NAME DATA AGE cm-files 6 4s sfqfs@sfq:~/k8s/ingress-nginx$ kubectl create configmap cm-literals --from-literal=key1=value1 configmap/cm-literals created sfqfs@sfq:~/k8s/ingress-nginx$ kubectl describe cm/cm-literals Name: cm-literals Namespace: default Labels: \u003cnone\u003e Annotations: \u003cnone\u003e Data ==== key1: ---- value1 BinaryData ==== Events: \u003cnone\u003e 使用 env.valueFrom.configMapKeyRef 获取 ConfigMap 中的值到环境变量 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 cat\u003c\u003cEOF | kubectl create -f - apiVersion: v1 kind: Pod metadata: name: pod-cm-env spec: restartPolicy: Never containers: - name: cm-test image: busybox command: [\"/bin/sh\", \"-c\", \"env | grep APP\"] env: # 定义环境变量名称 - name: APPLOGLEVEL # 定义环境变量对应的值 valueFrom: configMapKeyRef: # 指定环境变量来自cm/cm-appvars name: cm-appvars # 指定环境变量在 cm/cm-appvars 中的键 key: apploglevel - name: APPDATADIR valueFrom: configMapKeyRef: name: cm-appvars key: appdatadir EOF sfqfs@sfq:~/k8s$ kubectl logs pod/pod-cm-env APPDATADIR=/var/data APPLOGLEVEL=info xxdsdasdsa adsdsadasdasd sfqfs@sfq:~/k8s$ 使用 envFrom 获取 ConfigMap 中的值到环境变量 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 cat\u003c\u003cEOF | kubectl create -f - apiVersion: v1 kind: Pod metadata: name: pod-cm-env spec: restartPolicy: Never containers: - name: cm-test image: busybox command: [\"/bin/sh\", \"-c\", \"env | grep app\"] envFrom: - configMapRef: name: cm-appvars EOF sfqfs@sfq:~/k8s$ kubectl logs pod-cm-env apploglevel=info xxdsdasdsa adsdsadasdasd appdatadir=/var/data 使用 volumeMount 将 ConfigMap 挂载到容器内部的文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 cat\u003c\u003cEOF | kubectl create -f - apiVersion: v1 kind: Pod metadata: name: pod-cm-env spec: restartPolicy: Never containers: - name: cm-test image: busybox command: [\"/bin/sh\", \"-c\", \"ls /files; cat /files/*\"] volumeMounts: - name: files mountPath: /files volumes: # 定义 volume 名称 - name: files configMap: # 指定使用的 configmap 对象 name: cm-appvars # 将 key 对象的值以 path 指定的文件名进行挂载 # 如果不指定 items, 则默认将所有 key 对象的值以文件名的形式挂载 items: - key: apploglevel path: apploglevel.txt - key: appdatadir path: appdatadir.txt EOF sfqfs@sfq:~/k8s$ kubectl logs pod-cm-env appdatadir.txt apploglevel.txt /var/datainfo xxdsdasdsa adsdsadasdasd 使用 subPath 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 apiVersion: v1 kind: Pod metadata: name: pod-cm-env spec: restartPolicy: Never containers: - name: cm-test image: busybox command: [\"/bin/sh\", \"-c\", \"sleep 3600\"] volumeMounts: - name: files mountPath: /etc/apploglevel.txt subPath: apploglevel.txt volumes: # 定义 volume 名称 - name: files configMap: # 指定使用的 configmap 对象 name: cm-appvars items: - key: apploglevel path: apploglevel.txt EOF 热更新 ConfigMap 1 kubectl create cm cm-nginx --from-file=nginx.conf --dry-run -oyaml | kubectl replace -f - ","description":"","tags":["Kubernetes","ConfigMap","Secret"],"title":"K8S ConfigMap Secret","uri":"/posts/k8s-configmap-secret/"},{"categories":["Kuberneters"],"content":"Ingress 也是 k8s 的资源类型，ingress 用于实现用域名的方式访问 k8s 内部应用.\nChart: Helm 包，包含在 Kubernetes 集群内部运行应用程序，工具或者服务所需的所有资源定义。 Repository: 仓库，用来存放和共享 charts 的地方 Release: 运行在 k8s 集群中的 chart 示例。一个 chart 通常可以在一个集群中安装多次。每次安装都会创建一个新的 release。 ingress nginx 安装 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # 安装第三方仓库 helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx # 仓库查看 sfqfs@sfq:~/k8s$ helm repo list NAME URL ingress-nginx https://kubernetes.github.io/ingress-nginx test https://kubernetes.github.io/ingress-nginx # chart 查找 sfqfs@sfq:~/k8s$ helm search repo ingress-nginx NAME CHART VERSION APP VERSION DESCRIPTION ingress-nginx/ingress-nginx 4.5.2 1.6.4 Ingress controller for Kubernetes using NGINX a... test/ingress-nginx 4.5.2 1.6.4 Ingress controller for Kubernetes using NGINX a... # chart 下载到本地 sfqfs@sfq:~/k8s$ helm pull ingress-nginx/ingress-nginx sfqfs@sfq:~/k8s$ ls -l ingress-nginx-4.5.2.tgz -rw-r--r-- 1 sfqfs sfqfs 46009 Mar 13 14:01 ingress-nginx-4.5.2.tgz # 解压 sfqfs@sfq:~/k8s$ tar xf ingress-nginx-4.5.2.tgz sfqfs@sfq:~/k8s$ ls ingress-nginx CHANGELOG.md Chart.yaml OWNERS README.md README.md.gotmpl changelog changelog.md.gotmpl ci templates values.yaml sfqfs@sfq:~/k8s$ 修改 values.yaml\n修改镜像地址（image）\nhostNetwork: true （使用主机网络）\ndnsPolicy: ClusterFirstWithHostNet (和 hostNetwork: true 配合使用，优先使用集群内部的域名解析)\n使用 kind: DaemonSet 方式部署，可以固定到某些节点，外部负载均衡可以直接代理到这几台机器上\nnodeSelector 配置一下需要选择哪些节点\n1 2 3 nodeSelector: kubernetes.io/os: linux ingress: true resource 配置\nservice.type = LoadBalancer 改为 service.type = ClusterIP 因为本身就是通过宿主机的 IP 来访问了\n部署 ingress\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 sfqfs@sfq:~/k8s/ingress-nginx$ kubectl create ns ingress-nginx namespace/ingress-nginx created sfqfs@sfq:~/k8s/ingress-nginx$ helm install ingress-nginx -n ingress-nginx . NAME: ingress-nginx LAST DEPLOYED: Mon Mar 13 15:48:15 2023 NAMESPACE: ingress-nginx STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: The ingress-nginx controller has been installed. Get the application URL by running these commands: export POD_NAME=$(kubectl --namespace ingress-nginx get pods -o jsonpath=\"{.items[0].metadata.name}\" -l \"app=ingress-nginx,component=controller,release=ingress-nginx\") kubectl --namespace ingress-nginx port-forward $POD_NAME 8080:80 echo \"Visit http://127.0.0.1:8080 to access your application.\" An example Ingress that makes use of the controller: apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: example namespace: foo spec: ingressClassName: nginx rules: - host: www.example.com http: paths: - pathType: Prefix backend: service: name: exampleService port: number: 80 path: / # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: \u003cbase64 encoded cert\u003e tls.key: \u003cbase64 encoded key\u003e type: kubernetes.io/tls 给一个节点附加 ingress=true 来触发 pod ingress-nginx 安装\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # 查看当前 pod sfqfs@sfq:~/k8s$ kubectl get pod -A -owide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES kube-system coredns-5cfbb9f57c-l7xh5 1/1 Running 0 7d14h 10.42.0.3 node1.sfq.me \u003cnone\u003e \u003cnone\u003e kube-system local-path-provisioner-5f8bbd68f9-nhwbw 1/1 Running 0 7d14h 10.42.0.2 node1.sfq.me \u003cnone\u003e \u003cnone\u003e default nginx-deployment-7bf89ffbcd-d29qq 1/1 Running 0 7d5h 10.42.4.7 node4.sfq.me \u003cnone\u003e \u003cnone\u003e minio-dev minio 1/1 Running 0 7d 10.42.4.8 node4.sfq.me \u003cnone\u003e \u003cnone\u003e default nginx-deployment-7bf89ffbcd-5s6sm 1/1 Running 0 7d3h 10.42.1.9 node2.sfq.me \u003cnone\u003e \u003cnone\u003e kube-system metrics-server-847dcc659d-9dkrw 1/1 Running 1 (6d21h ago) 7d12h 10.42.1.7 node2.sfq.me \u003cnone\u003e \u003cnone\u003e default busybox 1/1 Running 73 (51m ago) 7d3h 10.42.1.8 node2.sfq.me \u003cnone\u003e \u003cnone\u003e # 在 node4 上安装 ingress-nginx sfqfs@sfq:~/k8s$ kubectl label node node4.sfq.me ingress=true node/node4.sfq.me labeled # 查看 ingress-nginx 安装成功 sfqfs@sfq:~/k8s$ kubectl get pod -A -owide -w NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES kube-system coredns-5cfbb9f57c-l7xh5 1/1 Running 0 7d14h 10.42.0.3 node1.sfq.me \u003cnone\u003e \u003cnone\u003e kube-system local-path-provisioner-5f8bbd68f9-nhwbw 1/1 Running 0 7d14h 10.42.0.2 node1.sfq.me \u003cnone\u003e \u003cnone\u003e default nginx-deployment-7bf89ffbcd-d29qq 1/1 Running 0 7d5h 10.42.4.7 node4.sfq.me \u003cnone\u003e \u003cnone\u003e minio-dev minio 1/1 Running 0 7d 10.42.4.8 node4.sfq.me \u003cnone\u003e \u003cnone\u003e default nginx-deployment-7bf89ffbcd-5s6sm 1/1 Running 0 7d3h 10.42.1.9 node2.sfq.me \u003cnone\u003e \u003cnone\u003e kube-system metrics-server-847dcc659d-9dkrw 1/1 Running 1 (6d21h ago) 7d12h 10.42.1.7 node2.sfq.me \u003cnone\u003e \u003cnone\u003e default busybox 1/1 Running 73 (53m ago) 7d3h 10.42.1.8 node2.sfq.me \u003cnone\u003e \u003cnone\u003e ingress-nginx ingress-nginx-controller-psnvh 0/1 Running 0 16s 10.42.4.10 node4.sfq.me \u003cnone\u003e \u003cnone\u003e ingress-nginx ingress-nginx-controller-psnvh 1/1 Running 0 20s 10.42.4.10 node4.sfq.me \u003cnone\u003e \u003cnone\u003e 在节点 node4 上可以看到宿主机监听的端口\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 [sfq@node4 ~]$ sudo netstat -ltpn | grep -E \"80|443\" [sudo] password for sfq: tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN 31687/nginx: master tcp 0 0 0.0.0.0:443 0.0.0.0:* LISTEN 31687/nginx: master tcp6 0 0 :::80 :::* LISTEN 31687/nginx: master tcp6 0 0 :::8443 :::* LISTEN 31631/nginx-ingress tcp6 0 0 :::443 :::* LISTEN 31687/nginx: master [sfq@node4 ~]$ ps aux | grep nginx root 18630 0.0 0.0 8920 3484 ? Ss Mar09 0:00 nginx: master process nginx -g daemon off; 101 18683 0.0 0.0 9308 1772 ? S Mar09 0:01 nginx: worker process 101 18684 0.0 0.0 9308 1776 ? S Mar09 0:01 nginx: worker process 101 31619 0.0 0.0 200 4 ? Ss 03:59 0:00 /usr/bin/dumb-init -- /nginx-ingress-controller --publish-service=ingress-nginx/ingress-nginx-controller --election-id=ingress-nginx-leader --controller-class=k8s.io/ingress-nginx --ingress-class=nginx --configmap=ingress-nginx/ingress-nginx-controller --validating-webhook=:8443 --validating-webhook-certificate=/usr/local/certificates/cert --validating-webhook-key=/usr/local/certificates/key 101 31631 0.2 0.7 754080 35480 ? Ssl 03:59 0:02 /nginx-ingress-controller --publish-service=ingress-nginx/ingress-nginx-controller --election-id=ingress-nginx-leader --controller-class=k8s.io/ingress-nginx --ingress-class=nginx --configmap=ingress-nginx/ingress-nginx-controller --validating-webhook=:8443 --validating-webhook-certificate=/usr/local/certificates/cert --validating-webhook-key=/usr/local/certificates/key 101 31687 0.0 0.7 146860 35432 ? S 03:59 0:00 nginx: master process /usr/bin/nginx -c /etc/nginx/nginx.conf 101 31692 0.0 0.7 158892 39704 ? Sl 03:59 0:00 nginx: worker process 101 31693 0.0 0.8 158996 40544 ? Sl 03:59 0:00 nginx: worker process 101 31694 0.0 0.5 144804 28456 ? S 03:59 0:00 nginx: cache manager process sfq 32352 0.0 0.0 112812 976 pts/0 S+ 04:16 0:00 grep --color=auto nginx [sfq@node4 ~]$ 配置 Nginx\n创建 servicea、serviceb\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 apiVersion: v1 kind: Service metadata: name: servicea namespace: ingress-nginx spec: selector: app: servicea ports: - name: http port: 8080 type: ClusterIP --- apiVersion: v1 kind: Service metadata: name: serviceb namespace: ingress-nginx spec: selector: app: serviceb ports: - name: http port: 8080 type: ClusterIP 配置 ingress-nginx(servicea.sfq.me, serviceb.sfq.me 的IP都配置到 ingress-nginx pod 使用主机网络的那台机器)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-myservicea namespace: ingress-nginx spec: rules: - host: servicea.sfq.me http: paths: - path: /servicea pathType: Prefix backend: service: name: servicea port: number: 8080 ingressClassName: nginx --- apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ingress-myserviceb namespace: ingress-nginx spec: rules: - host: serviceb.sfq.me http: paths: - path: /serviceb pathType: Prefix backend: service: name: serviceb port: number: 8080 ingressClassName: nginx 测试\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 # 访问 ingress-nginx 的 pod 对应宿主机 sfqfs@sfq:~/k8s$ kubectl get svc -n ingress-nginx -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR ingress-nginx-controller-admission ClusterIP 10.43.200.115 \u003cnone\u003e 443/TCP 3h37m app.kubernetes.io/component=controller,app.kubernetes.io/instance=ingress-nginx,app.kubernetes.io/name=ingress-nginx ingress-nginx-controller ClusterIP 10.43.163.111 \u003cnone\u003e 80/TCP,443/TCP 3h37m app.kubernetes.io/component=controller,app.kubernetes.io/instance=ingress-nginx,app.kubernetes.io/name=ingress-nginx servicea ClusterIP 10.43.130.154 \u003cnone\u003e 8080/TCP 12m app=servicea serviceb ClusterIP 10.43.97.133 \u003cnone\u003e 8080/TCP 4m25s app=serviceb,namespace=ingress-nginx # 访问服务A sfqfs@sfq:~/k8s$ curl -v servicea.sfq.me/servicea * Trying 192.168.0.204:80... * TCP_NODELAY set * Connected to servicea.sfq.me (192.168.0.204) port 80 (#0) \u003e GET /servicea HTTP/1.1 \u003e Host: servicea.sfq.me \u003e User-Agent: curl/7.68.0 \u003e Accept: */* \u003e * Mark bundle as not supporting multiuse \u003c HTTP/1.1 200 OK \u003c Date: Mon, 13 Mar 2023 11:28:53 GMT \u003c Content-Type: application/json; charset=utf-8 \u003c Content-Length: 18 \u003c Connection: keep-alive \u003c * Connection #0 to host servicea.sfq.me left intact {\"message\":\"pong\"} # 访问服务B sfqfs@sfq:~/k8s$ curl -v serviceb.sfq.me/serviceb * Trying 192.168.0.204:80... * TCP_NODELAY set * Connected to serviceb.sfq.me (192.168.0.204) port 80 (#0) \u003e GET /serviceb HTTP/1.1 \u003e Host: serviceb.sfq.me \u003e User-Agent: curl/7.68.0 \u003e Accept: */* \u003e * Mark bundle as not supporting multiuse \u003c HTTP/1.1 503 Service Temporarily Unavailable \u003c Date: Mon, 13 Mar 2023 11:29:01 GMT \u003c Content-Type: text/html \u003c Content-Length: 190 \u003c Connection: keep-alive \u003c \u003chtml\u003e \u003chead\u003e\u003ctitle\u003e503 Service Temporarily Unavailable\u003c/title\u003e\u003c/head\u003e \u003cbody\u003e \u003ccenter\u003e\u003ch1\u003e503 Service Temporarily Unavailable\u003c/h1\u003e\u003c/center\u003e \u003chr\u003e\u003ccenter\u003enginx\u003c/center\u003e \u003c/body\u003e \u003c/html\u003e * Connection #0 to host serviceb.sfq.me left intact ","description":"","tags":["Kuberneters","Ingress"],"title":"K8s Ingress","uri":"/posts/k8s-ingress/"},{"categories":null,"content":"工作流创建 创建 .github/workflows 目录 在 .github/workflows 目录下创建 main.yml 文件 在 main.yml 文件中添加如下内容 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 name: Github Actions Demo run-name: ${{ github.actor }} is testing out Github Actions on: [push] jobs: Explore-Github-Actions: runs-on: ubuntu-latest steps: - run: echo \"🎉 The job was automatically triggered by a ${{ github.event_name }} event.\" - run: echo \"🐧 This job is now running on a ${{ runner.os }} server hosted by GitHub!\" - run: echo \"🔎 The name of your branch is ${{ github.ref }} and your repository is ${{ github.repository }}.\" - name: Check out repository code uses: actions/checkout@v3 - run: echo \"💡 The ${{ github.repository }} repository has been cloned to the runner.\" - run: echo \"🖥️ The workflow is now ready to test your code on the runner.\" - name: List files in the repository run: | ls ${{ github.workspace }} - run: echo \"🍏 This job's status is ${{ job.status }}.\" 提交代码到 github 仓库 ","description":"","tags":null,"title":"Github Action","uri":"/posts/github-action/github-action/"}]